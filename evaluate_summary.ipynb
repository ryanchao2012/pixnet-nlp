{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import TokenQuestion, Word2VecSolver, KenLMSolver, CrawlerSolver, RetrievalSolver, Question, Solver\n",
    "import kenlm\n",
    "import string\n",
    "import collections\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "import shlex\n",
    "import re\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "\n",
    "# def match_words(sent, opts):\n",
    "    \n",
    "#     words = sent.split(' ')\n",
    "#     for word in words:\n",
    "#         poses.append(len(word)+poses[-1])\n",
    "#     sent_joined = ''.join(words)\n",
    "\n",
    "#     for opt in opts:\n",
    "#         start = sent_joined.find(opt)\n",
    "#         end = start + len(opt)\n",
    "#         yield words[poses.index(start):poses.index(end)]\n",
    "   \n",
    "# sent = '個人 覺得 natural 音波 淨 膚 儀 店 裡 的 擺設 屬於 鄉村 風 , 相當 有 味道 , 往往 一 個 很 普通 的 東西 混 著 擺 起來 就是 特別 好看 , 但 其實 試想 , 如果 把 任何 一 樣 東西 拿到 類似 辦公室 這 種 冰冷 色調 的 地方 擺 , 那 氛圍 很 容易 完全 走樣 , 實物 看起來 也 就 完全 不 生火 了 耶 ! ! ! 除了 東西 本身 , 整個 家 裡 的 色調 跟 氣氛 很 重要 阿 , 不然 再 有 風格 的 東西 拿回家 都 不 夠力 的 , 居家 裝潢 、 室 內 設計 就 是 這麼 回 事'\n",
    "# opt = '音波淨膚儀'\n",
    "# list(match_words(sent, [opt]))\n",
    "\n",
    "def reformat_gf_df(df, ref):\n",
    "    data2 = [x.lower().strip() for x in list(df['option'].values)]\n",
    "    q = ' and '.join(['{} == \"{}\"'.format(x, y) for x, y in zip('abcde', data2)])\n",
    "    try: \n",
    "        ans = list(ref.query(q)['ans'].values)\n",
    "        if len(ans) == 0:\n",
    "#             print(q)\n",
    "            return None\n",
    "    except:\n",
    "#         print(data2, q)\n",
    "        return None\n",
    "    content = None\n",
    "    for i, x in enumerate(data2):\n",
    "        s = df.iloc[i]['content'].lower().strip()\n",
    "        m = re.search(r'\\s?'.join([_x for _x in x]), s)\n",
    "        if m:\n",
    "            pat = m.group()\n",
    "            data2[i] = pat\n",
    "            if not content:\n",
    "                content = s.replace(pat, '︽⊙＿⊙︽') # ︽⊙＿⊙︽\n",
    "        else:\n",
    "#             print('!!',s, x)\n",
    "            return None\n",
    "\n",
    "    if not content:\n",
    "#         print(df.iloc[0]['content'], data2, q)\n",
    "        return None\n",
    "    data1 = [df.iloc[0]['no'], content]\n",
    "#     print(data1 + data2 + ans)\n",
    "    try: new_df = pd.Series(data1 + data2 + ans, index = ['no', 'content', 'a', 'b', 'c', 'd', 'e', 'ans'])\n",
    "    except: \n",
    "        print(data1 + data2 + ans)\n",
    "        return None\n",
    "    return new_df\n",
    "\n",
    "def reformat_df_test(df, ref):\n",
    "    pass\n",
    "\n",
    "ANS = ['a', 'b', 'c', 'd', 'e']\n",
    "WINDOW = 10\n",
    "VEC_SIZE = 100\n",
    "CATEGORY = {'國內旅遊': 'local_travel', '國外旅遊': 'global_travel', \n",
    "            '時尚流行': 'fashion', '美味食記': 'food', '美容彩妝': 'makeup', 'unkown': 'unkown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/tripper/Desktop/project/pixnet-nlp/data/DICT_CK+jieba_lower ...\n",
      "Loading model from cache /var/folders/4_/b_9vfbwx41g4t48jrzr0r4vm0000gn/T/jieba.uaab2b61a375629a86d8bcbcdd9e861b1.cache\n",
      "Loading model cost 1.214 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.set_dictionary('data/DICT_CK+jieba_lower')\n",
    "jieba.load_userdict('data/zhwiki-zh-clean')\n",
    "jieba.add_word('龐燮傍謝', freq=10, tag='xx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grep = RetrievalSolver('data/content-zh-lower.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crawler = CrawlerSolver(max_query_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kenlm_gf_o5 = KenLMSolver('lm-experiment/model/', 'gf-lm5.barpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kenlm_uni_o5 = KenLMSolver('lm-experiment/model/', 'uni-lm5.barpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kenlm_uni_o6 = KenLMSolver('lm-experiment/model/', 'uni-lm6.barpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kenlm_jieb_o5 = KenLMSolver('lm-experiment/model/', 'jb-lm5.barpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_jieba_cbow = Word2VecSolver('w2v-experiment/model/', 'jieba-cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_uni_cbow = Word2VecSolver('w2v-experiment/model/', 'uni-cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_gf_cbow = Word2VecSolver('w2v-experiment/model/', 'gf-cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_ptt_cbow = Word2VecSolver('w2v-experiment/model/', 'ptt-cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_set = pd.read_csv('question_sample/pre/raw_samples.csv')\n",
    "test_set = pd.read_csv('question_sample/official/hackathon_1000_lower.tsv', sep='\\t', names=['no','content', 'ans', 'a', 'b', 'c', 'd', 'e', 'ans_ref', 'url', 'level'], quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kenlm(guofoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'guofoo')\n",
    "    kenlm_gf_o5.solve(q)\n",
    "    if kenlm_gf_o5.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_gf_o5.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'jieba')\n",
    "    kenlm_jieb_o5.solve(q)\n",
    "    if kenlm_jieb_o5.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_jieb_o5.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 93.82%\n",
    "# total validation samples: 550\n",
    "# test accuracy: 85.80%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 55.59 sec\n",
    "# rate: 0.04 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec(gf+cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 76.56%\n",
      "total validation samples: 546\n",
      "test accuracy: 76.61%\n",
      "total test samples: 945\n",
      "elapsed time: 12.45 sec\n",
      "rate: 0.01 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "gf_cached_pred = pd.read_csv('cached/gf_validation.tsv', sep='\\t', names=['no','content', 'tag', 'option'])\n",
    "gf_cached_pred.fillna('', inplace=True)\n",
    "num = int(gf_cached_pred.shape[0] / 5)\n",
    "for i in range(num):\n",
    "    row = reformat_gf_df(gf_cached_pred[i * 5: i * 5 + 5], val_set)\n",
    "    if not type(row) == pd.core.series.Series: continue\n",
    "    q = TokenQuestion(row, tokenizer = 'guofoo')\n",
    "    w2v_gf_cbow.solve(q, solver='syn1neg')\n",
    "    if w2v_gf_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_gf_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "gf_cached_pred = pd.read_csv('cached/gf_test.tsv', sep='\\t', names=['no','content', 'tag', 'option'])\n",
    "gf_cached_pred.fillna('', inplace=True)\n",
    "num = int(gf_cached_pred.shape[0] / 5)\n",
    "\n",
    "for i in range(num):\n",
    "    row = reformat_gf_df(gf_cached_pred[i * 5: i * 5 + 5], test_set)\n",
    "    if not type(row) == pd.core.series.Series: continue\n",
    "    q = TokenQuestion(row, tokenizer = 'guofoo')\n",
    "    w2v_gf_cbow.solve(q, solver='syn1neg')\n",
    "    if w2v_gf_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_gf_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "\n",
    "# validation accuracy: 76.56%\n",
    "# total validation samples: 546\n",
    "# test accuracy: 76.61%\n",
    "# total test samples: 945\n",
    "# elapsed time: 12.45 sec\n",
    "# rate: 0.01 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec(ptt+cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 74.40%\n",
      "total test samples: 1000\n",
      "elapsed time: 1.33 sec\n",
      "rate: 0.00 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "\n",
    "temp = total\n",
    "# correct, total = 0, 0\n",
    "ptt_cached_pred = pd.read_csv('cached/ptt_test.csv', sep='\\t', names=['no','content', 'ans', 'a', 'b', 'c', 'd', 'e', 'ans_ref', 'url', 'level'], quoting=csv.QUOTE_NONE)\n",
    "ptt_cached_pred.fillna('', inplace=True)\n",
    "for (i1, r1), (i2, r2) in zip(test_set.iterrows(), ptt_cached_pred.iterrows()):\n",
    "    q = TokenQuestion(r1, tokenizer = 'cached', cached = r2.content.strip())\n",
    "    w2v_ptt_cbow.solve(q)\n",
    "    if w2v_ptt_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_ptt_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# test accuracy: 74.40%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 1.33 sec\n",
    "# rate: 0.00 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kenlm(unigram o6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 98.36%\n",
      "total validation samples: 550\n",
      "test accuracy: 93.00%\n",
      "total test samples: 1000\n",
      "elapsed time: 1.53 sec\n",
      "rate: 0.00 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    kenlm_uni_o6.solve(q)\n",
    "    if kenlm_uni_o6.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_uni_o6.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    kenlm_uni_o6.solve(q)\n",
    "    if kenlm_uni_o6.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_uni_o6.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 98.36%\n",
    "# total validation samples: 550\n",
    "# test accuracy: 93.00%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 1.53 sec\n",
    "# rate: 0.00 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kenlm(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 93.82%\n",
      "total validation samples: 550\n",
      "test accuracy: 85.80%\n",
      "total test samples: 1000\n",
      "elapsed time: 55.59 sec\n",
      "rate: 0.04 sec/sample\n"
     ]
    }
   ],
   "source": [
    "# kenlm(jieba)\n",
    "t = time()\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'jieba')\n",
    "    kenlm_jieb_o5.solve(q)\n",
    "    if kenlm_jieb_o5.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_jieb_o5.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'jieba')\n",
    "    kenlm_jieb_o5.solve(q)\n",
    "    if kenlm_jieb_o5.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_jieb_o5.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 93.82%\n",
    "# total validation samples: 550\n",
    "# test accuracy: 85.80%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 55.59 sec\n",
    "# rate: 0.04 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crawler retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 77.64%\n",
      "total validation samples: 550\n",
      "test accuracy: 62.00%\n",
      "total test samples: 1000\n",
      "elapsed time: 5743.00 sec\n",
      "rate: 3.71 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "cached_file = 'cached/crawler_validation.csv'\n",
    "pred = pd.DataFrame(columns=['no', 'prediction'])\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    crawler.solve(q)\n",
    "    pred = pred.append({'no': q.no, 'prediction': crawler.prediction}, ignore_index = True)\n",
    "    if crawler.prediction != Solver.UNSOLVED:\n",
    "        if crawler.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "pred['no'] = pred['no'].astype(int)\n",
    "pred.to_csv(cached_file, index=False)\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "cached_file = 'cached/crawler_test.csv'\n",
    "pred = pd.DataFrame(columns=['no', 'prediction'])\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    crawler.solve(q)\n",
    "    pred = pred.append({'no': q.no, 'prediction': crawler.prediction}, ignore_index = True)\n",
    "    if crawler.prediction != Solver.UNSOLVED:\n",
    "        if crawler.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "elapsed = time() - t\n",
    "pred['no'] = pred['no'].astype(int)\n",
    "pred.to_csv(cached_file, index=False)\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 77.64%\n",
    "# total validation samples: 550\n",
    "# test accuracy: 62.00%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 5743.00 sec\n",
    "# rate: 3.71 sec/sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 98.18%\n",
      "total validation samples: 550\n",
      "test accuracy: 92.90%\n",
      "total test samples: 1000\n",
      "elapsed time: 5.67 sec\n",
      "rate: 0.00 sec/sample\n"
     ]
    }
   ],
   "source": [
    "# kenlm(unigram-o5)\n",
    "t = time()\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    kenlm_uni_o5.solve(q)\n",
    "    if kenlm_uni_o5.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_uni_o5.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    kenlm_uni_o5.solve(q)\n",
    "    if kenlm_uni_o5.prediction != Solver.UNSOLVED:\n",
    "        if kenlm_uni_o5.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 98.18%\n",
    "# total validation samples: 550\n",
    "# test accuracy: 92.90%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 5.67 sec\n",
    "# rate: 0.00 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# direct word2vec(unigram+cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 63.19%\n",
      "total validation samples: 546\n",
      "test accuracy: 59.60%\n",
      "total test samples: 1000\n",
      "elapsed time: 2.32 sec\n",
      "rate: 0.00 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    w2v_uni_cbow.solve(q, uni_gram = True)\n",
    "    if w2v_uni_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_uni_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "    \n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'uni')\n",
    "    w2v_uni_cbow.solve(q, uni_gram = True)\n",
    "    if w2v_uni_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_uni_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 63.19%\n",
    "# total validation samples: 546\n",
    "# test accuracy: 59.60%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 2.32 sec\n",
    "# rate: 0.00 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grep + word2vec(jieba+cbow) aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 93.77%\n",
      "total validation samples: 546\n",
      "test accuracy: 86.80%\n",
      "total test samples: 1000\n",
      "elapsed time: 1.25 sec\n",
      "rate: 0.00 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "grep_cached_pred = pd.read_csv('cached/grep_validation.csv')\n",
    "grep_cached_pred.fillna('', inplace=True)\n",
    "for (i1, r1), (i2, r2) in zip(val_set.iterrows(), grep_cached_pred.iterrows()):\n",
    "    q = Question(r1)\n",
    "    if r2.prediction != str(Solver.UNSOLVED):\n",
    "        if r2.prediction == q.ans: correct += 1\n",
    "        elif len(r2.key) > 0:\n",
    "            w2v_jieba_cbow.solve(q, key = r2.key)\n",
    "            if w2v_jieba_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "grep_cached_pred = pd.read_csv('cached/grep_test.csv')\n",
    "grep_cached_pred.fillna('', inplace=True)\n",
    "for (i1, r1), (i2, r2) in zip(test_set.iterrows(), grep_cached_pred.iterrows()):\n",
    "    q = Question(r1)\n",
    "    if r2.prediction != str(Solver.UNSOLVED):\n",
    "        if r2.prediction == q.ans: correct += 1\n",
    "        elif len(r2.key) > 0:\n",
    "            w2v_jieba_cbow.solve(q, key = r2.key)\n",
    "            if w2v_jieba_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 93.77%\n",
    "# total validation samples: 546\n",
    "# test accuracy: 86.80%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 561.27 sec\n",
    "# rate: 0.36 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# direct word2vec(jieba+cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 79.30%\n",
      "total validation samples: 546\n",
      "test accuracy: 78.60%\n",
      "total test samples: 1000\n",
      "elapsed time: 2.37 sec\n",
      "rate: 0.00 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = TokenQuestion(row, 'jieba')\n",
    "    w2v_jieba_cbow.solve(q, solver = 'both')\n",
    "    if w2v_jieba_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_jieba_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = TokenQuestion(row, 'jieba')\n",
    "    w2v_jieba_cbow.solve(q, solver = 'both')\n",
    "    if w2v_jieba_cbow.prediction != Solver.UNSOLVED:\n",
    "        if w2v_jieba_cbow.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "\n",
    "elapsed = time() - t\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# == solver = syn1neg\n",
    "# validation accuracy: 79.85%\n",
    "# total validation samples: 546\n",
    "# test accuracy: 79.70%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 2.48 sec\n",
    "# rate: 0.00 sec/sample\n",
    "\n",
    "# == solver = syn0\n",
    "# validation accuracy: 65.75%\n",
    "# total validation samples: 546\n",
    "# test accuracy: 66.20%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 2.61 sec\n",
    "# rate: 0.00 sec/sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grep retrieval engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 93.77%\n",
      "total validation samples: 546\n",
      "test accuracy: 71.40%\n",
      "total test samples: 1000\n",
      "elapsed time: 4811.74 sec\n",
      "rate: 3.11 sec/sample\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "cached_file = 'cached/grep_validation.csv'\n",
    "pred = pd.DataFrame(columns=['no', 'key', 'prediction'])\n",
    "correct, total = 0, 0\n",
    "for index, row in val_set.iterrows():\n",
    "    q = Question(row)\n",
    "    grep.solve(q)\n",
    "    pred = pred.append({'no': q.no, 'key': grep.key, 'prediction': grep.prediction}, ignore_index = True)\n",
    "    if grep.prediction != Solver.UNSOLVED:\n",
    "        if grep.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "pred['no'] = pred['no'].astype(int)\n",
    "pred.to_csv(cached_file, index=False)\n",
    "print('validation accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total validation samples: {}'.format(total))\n",
    "\n",
    "temp = total\n",
    "cached_file = 'cached/grep_test.csv'\n",
    "pred = pd.DataFrame(columns=['no', 'key', 'prediction'])\n",
    "correct, total = 0, 0\n",
    "for index, row in test_set.iterrows():\n",
    "    q = Question(row)\n",
    "    grep.solve(q)\n",
    "    pred = pred.append({'no': q.no, 'key': grep.key, 'prediction': grep.prediction}, ignore_index = True)\n",
    "    if grep.prediction != Solver.UNSOLVED:\n",
    "        if grep.prediction == q.ans: correct += 1\n",
    "        total += 1\n",
    "elapsed = time() - t\n",
    "pred['no'] = pred['no'].astype(int)\n",
    "pred.to_csv(cached_file, index=False)\n",
    "\n",
    "print('test accuracy: {:.2f}%'.format(100*float(correct)/float(total)))\n",
    "print('total test samples: {}'.format(total))\n",
    "print('elapsed time: {:.2f} sec'.format(elapsed))\n",
    "print('rate: {:.2f} sec/sample'.format(float(elapsed) / float(temp + total)))\n",
    "\n",
    "# validation accuracy: 93.77%\n",
    "# total validation samples: 546\n",
    "# test accuracy: 71.40%\n",
    "# total test samples: 1000\n",
    "# elapsed time: 482.07 sec\n",
    "# rate: 0.31 sec/sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
